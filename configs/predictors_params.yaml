# CP-HOSfing Predictor Training Configuration
# Experiment: predictors
#
# This experiment trains hierarchical MLP classifiers for OS fingerprinting.
#
# Usage:
#   docker compose run --rm cphosfing predictors      # CPU mode
#   docker compose run --rm cphosfing-gpu predictors  # GPU mode
#
# GPU settings (use_amp, num_workers, use_multi_gpu) are automatically
# disabled when running in CPU mode.

# =============================================================================
# Dataset Paths (container paths - mount your data to /workspace/data)
# =============================================================================

# Input: Training dataset (created by data_split experiment)
dataset_train_path: /workspace/data/train.csv

# Input: Preprocessing maps (created by data_split experiment)
maps_path: /workspace/data/maps.plk

# =============================================================================
# Random Seed
# =============================================================================
seed: 2026

# =============================================================================
# Training Configuration
# =============================================================================

# Cross-validation settings
cv_splits: 5
max_configs: 1

# Which hierarchical levels to train models for
models_to_train:
  - family
  - major
  - leaf

# =============================================================================
# Hyperparameter Grids
# =============================================================================
# Optional: customize hyperparameter search space for each level
# If not specified, built-in granularity-specific defaults are used

hyperparameters:
  family:
    # Family level: fewer classes, simpler models
    hidden_dims: [[64, 32], [128, 64], [256, 128]]
    dropout: [0.1, 0.2, 0.3]
    lr: [0.0001, 0.0003, 0.001]
    weight_decay: [0.0, 0.00001, 0.0001]
    batch_size: [64, 128, 256]
    max_epochs: [100, 150, 200]
    patience: [20, 25, 30]
    use_bn: [true]
    use_onecycle: [true]
    grad_clip: [0.5, 1.0, 2.0]
    use_smote: [false]
    use_undersample: [true, false]
    n_features: [null]

  major:
    # Major level: medium complexity
    hidden_dims: [[64, 32], [128, 64], [256, 128], [512, 256]]
    dropout: [0.0, 0.1, 0.2, 0.3, 0.4]
    lr: [0.0001, 0.0003, 0.001, 0.003]
    weight_decay: [0.0, 0.00001, 0.0001, 0.001]
    batch_size: [32, 64, 128, 256]
    max_epochs: [100, 150, 200, 250]
    patience: [20, 25, 30]
    use_bn: [true]
    use_onecycle: [true]
    grad_clip: [0.5, 1.0, 2.0]
    use_smote: [false]
    use_undersample: [false]
    n_features: [null]

  leaf:
    # Leaf level: most classes, most complex models
    hidden_dims: [[64, 32], [128, 64], [256, 128], [512, 256], [1024, 512]]
    dropout: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]
    lr: [0.0001, 0.0003, 0.001, 0.003]
    weight_decay: [0.0, 0.00001, 0.0001, 0.001, 0.01]
    batch_size: [32, 64, 128, 256]
    max_epochs: [100, 150, 200, 250, 300]
    patience: [20, 25, 30]
    use_bn: [true]
    use_onecycle: [true]
    grad_clip: [0.5, 1.0, 2.0]
    use_smote: [false]
    use_undersample: [false]
    n_features: [null]

# =============================================================================
# GPU Configuration
# =============================================================================
# These settings are automatically disabled when running in CPU mode

use_amp: true              # Enable automatic mixed precision training
num_workers: 4             # Number of data loading workers
use_multi_gpu: true        # Enable multi-GPU training with DataParallel
